# Intro

It has been shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy. Surprising results were achieved on MNIST data.

But does distilling the knowledge in a neural network give an improved result when the data has larger number of classes. Lets find out using a wildlife data with a broad hyper dynamic range in [knowledge distillation](/knowledge_distillation.ipynb)

# References

https://keras.io/examples/vision/knowledge_distillation/

https://arxiv.org/abs/1503.02531

https://happywhale.com

